# ğŸ”ï¸ FrozenLake: Comparaison de 4 Approches d'Apprentissage par Renforcement

Ce projet compare **4 mÃ©thodes d'apprentissage par renforcement** sur l'environnement **FrozenLake** de Gymnasium (OpenAI Gym).

## ğŸ“‹ Table des MatiÃ¨res

- [Description](#description)
- [MÃ©thodes ImplÃ©mentÃ©es](#mÃ©thodes-implÃ©mentÃ©es)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du Code](#structure-du-code)
- [RÃ©sultats Attendus](#rÃ©sultats-attendus)
- [ParamÃ¨tres Configurables](#paramÃ¨tres-configurables)
- [DÃ©pendances](#dÃ©pendances)


---

## ğŸ“– Description

**FrozenLake** est un environnement classique de Reinforcement Learning oÃ¹ un agent doit naviguer sur un lac gelÃ© de 4x4 cases pour atteindre un objectif sans tomber dans les trous.

```
ğŸŸ¢ = Start (DÃ©part)
â„ï¸  = Frozen (Glace - sÃ»r)
ğŸ•³ï¸  = Hole (Trou - Ã©chec)
ğŸ¯ = Goal (Objectif - rÃ©ussite)
```

**CaractÃ©ristiques :**
- Environnement stochastique (`is_slippery=True`) : l'agent peut glisser
- 16 Ã©tats possibles (4x4 grille)
- 4 actions : â¬…ï¸ Gauche, â¬‡ï¸ Bas, â¡ï¸ Droite, â¬†ï¸ Haut
- RÃ©compense : +1 si objectif atteint, 0 sinon

---

## ğŸ¯ MÃ©thodes ImplÃ©mentÃ©es

### 1ï¸âƒ£ **Q-Learning (Tabular)**

**Principe :** Apprentissage basÃ© sur une table Q stockant les valeurs Q(s,a) pour chaque paire Ã©tat-action.

**CaractÃ©ristiques :**
- Table Q de dimension 16Ã—4
- Mise Ã  jour : `Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]`
- Aucune approximation de fonction
- MÃ©thode classique et simple

**HyperparamÃ¨tres :**
- Learning rate (Î±) : 0.1
- Discount factor (Î³) : 0.95
- Epsilon decay : 0.995

---

### 2ï¸âƒ£ **Deep Reinforcement Learning**

**Principe :** RÃ©seau de neurones simple (MLP avec 1 couche cachÃ©e) qui approxime la fonction Q.

**Architecture :**
```
Input (16) â†’ Hidden Layer (32, ReLU) â†’ Output (4)
```

**CaractÃ©ristiques :**
- Apprentissage direct sans experience replay
- Activation ReLU
- Backpropagation manuelle
- One-hot encoding des Ã©tats

**HyperparamÃ¨tres :**
- Hidden size : 32 neurones
- Learning rate : 0.01
- Discount factor (Î³) : 0.95

---

### 3ï¸âƒ£ **DQN (Deep Q-Network)**

**Principe :** Extension du Deep RL avec **Experience Replay** pour stabiliser l'apprentissage.

**Architecture :**
```
Input (16) â†’ Hidden Layer (64, ReLU) â†’ Output (4)
```

**CaractÃ©ristiques :**
- **Experience Replay Buffer** : stocke 2000 transitions
- **Batch Learning** : entraÃ®nement sur mini-batches de 32
- Meilleure stabilitÃ© que Deep RL basique
- DÃ©corrÃ©lation des expÃ©riences

**HyperparamÃ¨tres :**
- Hidden size : 64 neurones
- Learning rate : 0.001
- Buffer size : 2000
- Batch size : 32

---

### 4ï¸âƒ£ **Baseline DQN (Stable-Baselines3)**

**Principe :** ImplÃ©mentation professionnelle et optimisÃ©e de DQN par Stable-Baselines3.

**CaractÃ©ristiques :**
- Architecture MLP optimisÃ©e automatiquement
- ImplÃ©mentation testÃ©e et validÃ©e
- Nombreuses optimisations (target network, etc.)
- API simple et efficace

**HyperparamÃ¨tres :**
- Policy : "MlpPolicy"
- Buffer size : 10000
- Learning starts : 100
- Exploration fraction : 0.5

---

## ğŸ› ï¸ Installation

### PrÃ©requis

- Python 3.8+
- pip

### Installation des dÃ©pendances

```bash
# Cloner le repository (ou tÃ©lÃ©charger les fichiers)
git clone <votre-repo>
cd frozenlake-rl-comparison

# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv

# Activer l'environnement virtuel
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Installer les dÃ©pendances
pip install gymnasium numpy matplotlib stable-baselines3
```

### Installation alternative (sans Stable-Baselines3)

Si vous ne souhaitez pas utiliser Stable-Baselines3 :

```bash
pip install gymnasium numpy matplotlib
```

Le programme dÃ©tectera automatiquement l'absence de la bibliothÃ¨que et sautera la mÃ©thode 4.

---

## ğŸš€ Utilisation

### ExÃ©cution du programme complet

```bash
python frozen_lake_comparison.py
```

### Ce qui se passe :

1. **EntraÃ®nement** de chaque agent sur 1000 Ã©pisodes
2. **Test** de chaque agent sur 100 Ã©pisodes
3. **Affichage** des taux de succÃ¨s
4. **GÃ©nÃ©ration** d'un graphique comparatif (`comparison_results.png`)

### Sortie attendue

```
ğŸ”ï¸  FROZEN LAKE - COMPARAISON DE 4 APPROCHES
State size: 16, Action size: 4

======================================================================
1ï¸âƒ£  Q-LEARNING (TABLE)
======================================================================
ğŸŸ¦ Q-Learning Agent: Table 16x4
...
Episode 100/1000 | Avg Reward: 0.120 | Epsilon: 0.366
...

ğŸ¯ Test: Q-Learning (100 Ã©pisodes)
âœ… Taux de succÃ¨s: 65.0% (65/100)

[RÃ©pÃ©tÃ© pour les 4 mÃ©thodes]

======================================================================
ğŸ“Š RÃ‰SULTATS FINAUX
======================================================================
Q-Learning          : 65.0% de succÃ¨s
Deep RL             : 45.0% de succÃ¨s
DQN                 : 72.0% de succÃ¨s
Baseline DQN        : 78.0% de succÃ¨s
```

---

## ğŸ“ Structure du Code

```
frozen_lake_comparison.py
â”œâ”€â”€ Classes
â”‚   â”œâ”€â”€ QLearningAgent          # Q-Learning avec table
â”‚   â”œâ”€â”€ NeuralNetwork            # RÃ©seau de neurones custom
â”‚   â”œâ”€â”€ DeepRLAgent              # Deep RL sans replay
â”‚   â””â”€â”€ DQNAgent                 # DQN avec experience replay
â”‚
â”œâ”€â”€ Fonctions Utilitaires
â”‚   â”œâ”€â”€ state_to_vector()        # Conversion one-hot
â”‚   â”œâ”€â”€ train_agent()            # EntraÃ®nement gÃ©nÃ©rique
â”‚   â”œâ”€â”€ train_baseline_dqn()     # EntraÃ®nement Baseline
â”‚   â”œâ”€â”€ test_agent()             # Test des agents
â”‚   â””â”€â”€ plot_comparison()        # Visualisation
â”‚
â””â”€â”€ Main
    â””â”€â”€ ExÃ©cution sÃ©quentielle des 4 mÃ©thodes
```

---

## ğŸ“Š RÃ©sultats Attendus

### Performances Typiques (1000 Ã©pisodes)

| MÃ©thode | Taux de SuccÃ¨s | Temps d'EntraÃ®nement | StabilitÃ© |
|---------|---------------|---------------------|-----------|
| **Q-Learning** | 60-70% | â­â­â­â­â­ | â­â­â­â­ |
| **Deep RL** | 40-55% | â­â­â­â­ | â­â­ |
| **DQN** | 65-75% | â­â­â­ | â­â­â­â­ |
| **Baseline DQN** | 70-80% | â­â­ | â­â­â­â­â­ |

### Observations

- **Q-Learning** : Simple et efficace pour petits espaces d'Ã©tats
- **Deep RL** : Moins stable sans experience replay
- **DQN** : Bon compromis performance/complexitÃ©
- **Baseline DQN** : Meilleure performance grÃ¢ce aux optimisations

---

## âš™ï¸ ParamÃ¨tres Configurables

### Dans le code principal (`__main__`)

```python
EPISODES = 1000  # Nombre d'Ã©pisodes d'entraÃ®nement
```

### Pour chaque agent

**Q-Learning :**
```python
learning_rate = 0.1      # Taux d'apprentissage
gamma = 0.95             # Discount factor
epsilon_decay = 0.995    # DÃ©croissance de l'exploration
```

**Deep RL / DQN :**
```python
hidden_size = 32         # Taille de la couche cachÃ©e
learning_rate = 0.01     # Taux d'apprentissage
buffer_size = 2000       # Taille du buffer (DQN seulement)
batch_size = 32          # Taille des batches (DQN seulement)
```

**Baseline DQN :**
```python
buffer_size = 10000      # Taille du replay buffer
learning_starts = 100    # Steps avant apprentissage
exploration_fraction = 0.5  # Fraction d'exploration
```

---

## ğŸ“¦ DÃ©pendances

```txt
gymnasium>=0.29.0
numpy>=1.24.0
matplotlib>=3.7.0
stable-baselines3>=2.0.0  # Optionnel
```

### Installation complÃ¨te

```bash
pip install gymnasium numpy matplotlib stable-baselines3
```

---

## ğŸ“ Concepts ClÃ©s

### Experience Replay
Stockage des transitions (s, a, r, s') dans un buffer pour dÃ©corrÃ©ler les expÃ©riences et amÃ©liorer la stabilitÃ©.

### Epsilon-Greedy
StratÃ©gie d'exploration : choisir une action alÃ©atoire avec probabilitÃ© Îµ, sinon l'action optimale.

### One-Hot Encoding
Conversion d'un Ã©tat discret en vecteur binaire pour les rÃ©seaux de neurones.

### Discount Factor (Î³)
PondÃ©ration des rÃ©compenses futures : rÃ©compense totale = r + Î³r' + Î³Â²r'' + ...

---

## ğŸ› Troubleshooting

### Erreur : Module 'stable_baselines3' not found
```bash
pip install stable-baselines3
```

### Performances faibles
- Augmenter le nombre d'Ã©pisodes (2000-5000)
- Ajuster le learning rate
- Modifier la taille du rÃ©seau de neurones

### Pas de graphique affichÃ©
VÃ©rifiez que matplotlib est installÃ© et que vous n'Ãªtes pas en environnement headless.

---






## ğŸ“ Notes

- FrozenLake est un environnement stochastique, les rÃ©sultats peuvent varier
- Pour des rÃ©sultats reproductibles, fixez le seed : `np.random.seed(42)`
- L'entraÃ®nement complet prend environ 2-5 minutes selon votre machine

---


